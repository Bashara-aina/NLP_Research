{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d88914-58c7-447c-9504-0086422f9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 18:25:19.514786: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-30 18:25:21.922359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 18:25:24.427099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from torch import nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578eb2c1-38ca-4a3a-a86f-69f461a083c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_us_reviews (/home/z123010/.cache/huggingface/datasets/amazon_us_reviews/Apparel_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff3e242f51745e19dea1285fc9f646c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"amazon_us_reviews\", \"Apparel_v1_00\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Limit the dataset to the first 100,000 rows\n",
    "train_data = train_data.select(range(100000))\n",
    "\n",
    "df = train_data.to_pandas()  # Convert the dataset to a Pandas DataFrame\n",
    "df = df[['customer_id', 'review_headline', 'review_body', 'star_rating']]  # Select specific columns\n",
    "df.columns = ['customer_id', 'review_headline', 'review_body', 'star_rating']  # Rename the selected columns\n",
    "df.set_index('customer_id', inplace=True)\n",
    "df.head()  # Display the first few rows of the DataFrame\n",
    "df['sentiment'] = df['star_rating'].map({5: 'good', 4: 'good', 3: 'neutral', 2: 'bad', 1: 'bad'})\n",
    "possible_labels = df.sentiment.unique() #Get unique category labels from the DataFrame column 'category'\n",
    "df['label'] = df.sentiment.replace(label_dict)\n",
    "#Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df.index.values,\n",
    "    df.label.values,\n",
    "    test_size=0.15,\n",
    "    random_state=17,\n",
    "    stratify=df.label.values\n",
    ")\n",
    "df['data_type'] = ['not_set']*df.shape[0] #Set a new column 'data_type' for later data split\n",
    "#Set the 'data_type' column of the dataframe for training and validation data\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70acbcc-81a3-443f-8779-65872aab68e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star_rating</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>2160</td>\n",
       "      <td>2160</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>5463</td>\n",
       "      <td>5463</td>\n",
       "      <td>5463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1228</td>\n",
       "      <td>1228</td>\n",
       "      <td>1228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>8439</td>\n",
       "      <td>8439</td>\n",
       "      <td>8439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>14202</td>\n",
       "      <td>14202</td>\n",
       "      <td>14202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>3561</td>\n",
       "      <td>3561</td>\n",
       "      <td>3561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>41279</td>\n",
       "      <td>41279</td>\n",
       "      <td>41279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>12095</td>\n",
       "      <td>12095</td>\n",
       "      <td>12095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             review_headline  review_body  sentiment\n",
       "star_rating label data_type                                         \n",
       "1           2     train                 9581         9581       9581\n",
       "                  val                   2160         2160       2160\n",
       "2           2     train                 5463         5463       5463\n",
       "                  val                   1228         1228       1228\n",
       "3           1     train                 8439         8439       8439\n",
       "                  val                   1992         1992       1992\n",
       "4           0     train                14202        14202      14202\n",
       "                  val                   3561         3561       3561\n",
       "5           0     train                41279        41279      41279\n",
       "                  val                  12095        12095      12095"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df.index.values,\n",
    "    df.label.values,\n",
    "    test_size=0.15,\n",
    "    random_state=17,\n",
    "    stratify=df.label.values\n",
    ")\n",
    "df['data_type'] = ['not_set'] * df.shape[0]\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "df.groupby(['star_rating', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24bd593-def4-4818-9bef-1e1192959f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa52ab6-2932-4724-ae59-094d2f40eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train_headline = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].review_headline.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='longest',\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_train_body = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].review_body.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='longest',\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_train_headline = encoded_data_train_headline['input_ids']\n",
    "attention_masks_train_headline = encoded_data_train_headline['attention_mask']\n",
    "\n",
    "input_ids_train_body = encoded_data_train_body['input_ids']\n",
    "attention_masks_train_body = encoded_data_train_body['attention_mask']\n",
    "\n",
    "input_ids_train = torch.cat((input_ids_train_headline, input_ids_train_body), dim=1)\n",
    "attention_masks_train = torch.cat((attention_masks_train_headline, attention_masks_train_body), dim=1)\n",
    "\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "\n",
    "encoded_data_val_headline = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].review_headline.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='longest',\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val_body = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].review_body.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='longest',\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_val_headline = encoded_data_val_headline['input_ids']\n",
    "attention_masks_val_headline = encoded_data_val_headline['attention_mask']\n",
    "\n",
    "input_ids_val_body = encoded_data_val_body['input_ids']\n",
    "attention_masks_val_body = encoded_data_val_body['attention_mask']\n",
    "\n",
    "input_ids_val = torch.cat((input_ids_val_headline, input_ids_val_body), dim=1)\n",
    "attention_masks_val = torch.cat((attention_masks_val_headline, attention_masks_val_body), dim=1)\n",
    "\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce4a4351-d496-4fa0-8984-c63ac219ce05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the data loaders\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      4\u001b[0m dataloader_train \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mdataset_train\u001b[49m,\n\u001b[1;32m      6\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mRandomSampler(dataset_train),\n\u001b[1;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m dataloader_val \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     11\u001b[0m     dataset_val,\n\u001b[1;32m     12\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mRandomSampler(dataset_val),\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the data loaders\n",
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    sampler=RandomSampler(dataset_train),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    sampler=RandomSampler(dataset_val),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de8b195-fd5c-4ff4-bfef-96b277aa05a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509d1d4d-1108-44bc-9236-0e957d8f2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RCNN model\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_size, num_classes, num_layers=1):\n",
    "        super(RCNN, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.lstm = nn.LSTM(bert_model.config.hidden_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(2 * hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        def custom_forward(input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bert_embeddings = outputs.last_hidden_state\n",
    "            lstm_out, _ = self.lstm(bert_embeddings.permute(1, 0, 2))\n",
    "            features = torch.cat([lstm_out[:, -1, :self.lstm.hidden_size], lstm_out[:, 0, self.lstm.hidden_size:]], dim=1)\n",
    "            logits = self.fc(features)\n",
    "            return logits\n",
    "\n",
    "        return checkpoint.checkpoint(custom_forward, input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120d31e1-e853-42a1-acd4-f2c2993259f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c8cd4e9-9f3a-4eb1-b931-b812b4ded7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RCNN model\n",
    "hidden_size = 256\n",
    "num_classes = len(label_dict)\n",
    "model = RCNN(bert_model, hidden_size, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d9ba8e0-229c-45e4-8e2a-9c36eb55c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z123010/.local/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742c551f-c307-4574-8a1c-c60a18b368a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.max_split_size_mb = 2200\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "data = torch.randn([1, 3, 470, 446], dtype=torch.float, device='cuda', requires_grad=True)\n",
    "net = torch.nn.Conv2d(3, 64, kernel_size=[7, 7], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)\n",
    "net = net.cuda().float()\n",
    "out = net(data)\n",
    "out.backward(torch.randn_like(out))\n",
    "torch.cuda.synchronize()\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371f71c9-ff95-4dd8-9598-b29d74b6b34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a19e159c15544598b52a80b960d97bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     14\u001b[0m loss_train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[43mdataloader_train\u001b[49m,\n\u001b[1;32m     17\u001b[0m                     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{:1d}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch),\n\u001b[1;32m     18\u001b[0m                     leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_train' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Epoch {}/{}\".format(epoch + 1, epochs), leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Epoch {} average loss: {:.4f}\".format(epoch + 1, avg_loss))\n",
    "\n",
    "# Evaluation on validation set\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to('cpu').numpy()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        val_predictions.extend(predicted_labels.to('cpu').numpy())\n",
    "        val_true_labels.extend(labels)\n",
    "val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "val_precision = precision_score(val_true_labels, val_predictions, average='weighted', zero_division=0)\n",
    "val_f1 = f1_score(val_true_labels, val_predictions, average='weighted', zero_division=0)\n",
    "print(\"Validation Accuracy: {:.4f}\".format(val_accuracy))\n",
    "print(\"Validation Precision: {:.4f}\".format(val_precision))\n",
    "print(\"Validation F1-Score: {:.4f}\".format(val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a6d64-f310-4e25-a0fa-0f51e634ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "total_training_time = 0\n",
    "\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train,\n",
    "                        desc='Epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                        disable=False)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2],\n",
    "        }\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_training_time = end_time - start_time\n",
    "    total_training_time += epoch_training_time\n",
    "\n",
    "    torch.save(model.state_dict(), f'Models/Body/finetuned_bert_ft_epoch{epoch}.model')\n",
    "\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "\n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "\n",
    "    # Convert predictions to discrete labels\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    val_f1 = f1_score(true_vals, predictions, average='weighted')\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (weighted): {val_f1}')\n",
    "\n",
    "    val_accuracy = accuracy_score(true_vals, predictions)\n",
    "    val_precision = precision_score(true_vals, predictions, average='weighted', zero_division=1)\n",
    "\n",
    "    accuracy_list.append(val_accuracy)\n",
    "    precision_list.append(val_precision)\n",
    "\n",
    "total_time_minutes = total_training_time / 60\n",
    "tqdm.write(f'\\nTotal training time: {total_time_minutes} minutes')\n",
    "\n",
    "final_accuracy = accuracy_list[-1]\n",
    "final_precision = precision_list[-1]\n",
    "tqdm.write(f'Final Accuracy: {final_accuracy}')\n",
    "tqdm.write(f'Final Precision: {final_precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99041603-040a-4d2a-83bf-0962dfbe90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training samples:\", len(input_ids_train))\n",
    "print(\"Number of validation samples:\", len(input_ids_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5e074-94ec-4ffa-9389-9a8ce748e990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
